{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation"
      ],
      "metadata": {
        "id": "Wyml0xMUKIGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install panns_inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhUA45ucNRKT",
        "outputId": "a8fd9bd3-7347-4c43-9515-d40ca631365d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting panns_inference\n",
            "  Downloading panns_inference-0.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from panns_inference) (3.8.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from panns_inference) (0.10.2.post1)\n",
            "Collecting torchlibrosa (from panns_inference)\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->panns_inference) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->panns_inference) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->panns_inference) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->panns_inference) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->panns_inference) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->panns_inference) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->panns_inference) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->panns_inference) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->panns_inference) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (2024.12.14)\n",
            "Downloading panns_inference-0.1.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: torchlibrosa, panns_inference\n",
            "Successfully installed panns_inference-0.1.1 torchlibrosa-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "from google.colab import files\n",
        "from panns_inference import AudioTagging, SoundEventDetection\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "gpIOxtEdMFkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modèle sur CNN14\n",
        "audio_tagging = AudioTagging(checkpoint_path=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyD_4ynSOs18",
        "outputId": "00901fc9-c97d-4859-ad90-0c62e9b35570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/panns_inference/inference.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU number: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjkYdHDj68MV",
        "outputId": "3e2c62af-3f0a-4dd4-80de-3dae5010f920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'annotations_clean_and_noised.xlsx'\n",
        "\n",
        "# Charger le fichier Excel\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Ajouter une colonne pour les embeddings\n",
        "df[\"embeddings\"] = None\n",
        "\n",
        "# Ajouter chaque embeddings de chaque instance dans le dataframe\n",
        "for index, row in df.iterrows():\n",
        "    try:\n",
        "      # Charger la donnée audio depuis la source\n",
        "      wav_path = '/content/' + str(row['filename'])\n",
        "\n",
        "      # Obtenir le bon format\n",
        "      waveform, sample_rate = torchaudio.load(wav_path)\n",
        "\n",
        "      # Applique le modèle d'extraction d'embeddings à des données audio WAV\n",
        "      _, embedding = audio_tagging.inference(waveform)\n",
        "\n",
        "      # Ajouter le résultat dans le DataFrame\n",
        "      df.at[index, \"embeddings\"] = embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur\")"
      ],
      "metadata": {
        "id": "8hvtMBAJj0Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split des données\n",
        "def split_data(df):\n",
        "    train_df = df[df[\"fold\"].isin([1, 2, 3, 4, 5, 6])]\n",
        "    valid_df = df[df[\"fold\"] == 7]\n",
        "\n",
        "    return train_df, valid_df\n",
        "\n",
        "train_df, valid_df = split_data(df)"
      ],
      "metadata": {
        "id": "iQ5CXEBHvsCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset des données pour le dataloader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.features = torch.tensor(data[\"embeddings\"].tolist(), dtype=torch.float32)\n",
        "        self.labels = torch.tensor(data[\"target\"].tolist(), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Création des datasets\n",
        "train_dataset = CustomDataset(train_df)\n",
        "valid_dataset = CustomDataset(valid_df)"
      ],
      "metadata": {
        "id": "h4D3h9avv1QR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7ca1f7-01c1-4ec0-d9c9-0e6549310412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2485106cd6a5>:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  self.features = torch.tensor(data[\"embeddings\"].tolist(), dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création des DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "f3E9jhVdv_UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modèle"
      ],
      "metadata": {
        "id": "gz31egCMKLpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrée : Un vecteur de taille 2048 (représentant un embedding)\n",
        "# Couche cachée : des couches denses avec 128 neurones et une activation ReLU (pleinement connectées)\n",
        "# Couche sortie : nombre de classe\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size = 2048, num_classes = 5):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, 128)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.out = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = MLP(input_size=2048, num_classes= 5).to('cuda')\n",
        "\n",
        "# Résumé de l'architecture\n",
        "summary(model, input_size=(2048,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjM9evvZwDLE",
        "outputId": "243b5995-efc6-4d9d-ff47-e22ffa3d9f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 128]         262,272\n",
            "              ReLU-2                  [-1, 128]               0\n",
            "            Linear-3                  [-1, 128]          16,512\n",
            "              ReLU-4                  [-1, 128]               0\n",
            "            Linear-5                  [-1, 128]          16,512\n",
            "              ReLU-6                  [-1, 128]               0\n",
            "            Linear-7                    [-1, 5]             645\n",
            "================================================================\n",
            "Total params: 295,941\n",
            "Trainable params: 295,941\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 1.13\n",
            "Estimated Total Size (MB): 1.14\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrainement"
      ],
      "metadata": {
        "id": "wQwZFioEKRWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5):\n",
        "        self.patience = patience\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, valid_loss):\n",
        "        if valid_loss < self.best_loss:\n",
        "            self.best_loss = valid_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "Xhx-iHU6Uv-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation d'early stopping\n",
        "early_stopping = EarlyStopping(patience=5)"
      ],
      "metadata": {
        "id": "gkmUIJVcU5Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrainement\n",
        "epochs = 100\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Entraînement\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features.to('cuda')).squeeze(1)\n",
        "\n",
        "        # Calcul de la loss\n",
        "        loss = criterion(outputs, labels.to('cuda'))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Mise à jour des métriques d'entraînement\n",
        "        total_train_loss += loss.item()\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        correct_train += (predictions.to('cuda') == labels.to('cuda')).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_loss = total_train_loss / len(train_loader)\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_valid_loss = 0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in valid_loader:\n",
        "            outputs = model(features.to('cuda')).squeeze(1)\n",
        "\n",
        "            # Calcul de la loss\n",
        "            loss = criterion(outputs, labels.to('cuda'))\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "            # Mise à jour des métriques de validation\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct_valid += (predictions == labels.to('cuda')).sum().item()\n",
        "            total_valid += labels.size(0)\n",
        "\n",
        "    valid_loss = total_valid_loss / len(valid_loader)\n",
        "    valid_accuracy = correct_valid / total_valid\n",
        "\n",
        "    # Métriques pour l'époque en cours\n",
        "    print(f\"Epoch {epoch + 1}:\")\n",
        "    print(f\"Train Loss: {train_loss:}, Train Accuracy: {train_accuracy:}\")\n",
        "    print(f\"Valid Loss: {valid_loss:}, Valid Accuracy: {valid_accuracy:}\")\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(valid_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping stopped the training!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv9HrivmAOAe",
        "outputId": "441d6c86-d025-462b-bf3d-2d617e6f1d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "Train Loss: 1.2592209978029132, Train Accuracy: 0.47860304968027545\n",
            "Valid Loss: 1.2022551417350769, Valid Accuracy: 0.5331230283911672\n",
            "Epoch 2:\n",
            "Train Loss: 1.1905184015631676, Train Accuracy: 0.49090014756517464\n",
            "Valid Loss: 1.1789111971855164, Valid Accuracy: 0.5394321766561514\n",
            "Epoch 3:\n",
            "Train Loss: 1.135809998959303, Train Accuracy: 0.5012297097884899\n",
            "Valid Loss: 1.1316990077495575, Valid Accuracy: 0.5425867507886435\n",
            "Epoch 4:\n",
            "Train Loss: 1.0913202753290534, Train Accuracy: 0.5189375307427447\n",
            "Valid Loss: 1.1435083210468293, Valid Accuracy: 0.5488958990536278\n",
            "Epoch 5:\n",
            "Train Loss: 1.0679459497332573, Train Accuracy: 0.5302508607968519\n",
            "Valid Loss: 1.1055566728115083, Valid Accuracy: 0.5772870662460567\n",
            "Epoch 6:\n",
            "Train Loss: 1.0554489009082317, Train Accuracy: 0.5081160846040335\n",
            "Valid Loss: 1.0553266167640687, Valid Accuracy: 0.5772870662460567\n",
            "Epoch 7:\n",
            "Train Loss: 1.0454010488465428, Train Accuracy: 0.5282833251352681\n",
            "Valid Loss: 1.0601918756961823, Valid Accuracy: 0.5741324921135647\n",
            "Epoch 8:\n",
            "Train Loss: 1.0339205153286457, Train Accuracy: 0.5258239055582883\n",
            "Valid Loss: 1.1256044924259185, Valid Accuracy: 0.5520504731861199\n",
            "Epoch 9:\n",
            "Train Loss: 1.0238042501732707, Train Accuracy: 0.5386128873585834\n",
            "Valid Loss: 1.0782961428165436, Valid Accuracy: 0.5488958990536278\n",
            "Epoch 10:\n",
            "Train Loss: 1.0175376376137137, Train Accuracy: 0.5361534677816036\n",
            "Valid Loss: 1.103602147102356, Valid Accuracy: 0.48580441640378547\n",
            "Epoch 11:\n",
            "Train Loss: 0.995464488863945, Train Accuracy: 0.5371372356123955\n",
            "Valid Loss: 1.0905680656433105, Valid Accuracy: 0.5678233438485805\n",
            "Early stopping stopped the training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Sauvegarder le modèle entrainé"
      ],
      "metadata": {
        "id": "zamYdidGKag0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"MLP_on_CNN14_clean_and_noised.pth\")"
      ],
      "metadata": {
        "id": "fDPzHiL9eiSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Téléchargement du modèle\n",
        "from google.colab import files\n",
        "files.download('MLP_on_CNN14_clean_and_noised.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "U_ppwxNuekxU",
        "outputId": "791a5f47-d3b8-46cc-92b2-2e51eec5a134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0548a747-884d-4f6a-8acc-46e8e7d28de6\", \"MLP_on_CNN14_clean_and_noised.pth\", 1187336)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}